{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imageio import imread\n",
    "\n",
    "from keras.applications import InceptionV3\n",
    "from keras import Model\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem description\n",
    "\n",
    "Our goal is to show how neural networks behave during automated image captioning. We aim to understand what are the composants of the neural network that are activated and that participates in the recognition of an \"object\" in the image. \n",
    "\n",
    "In a Second step, we will try to see if we can reduce the size of the neural network using this information to produce a smaller neural net with the same behaviour as the first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Our aim is not accuracy, we want a neural network with a decent accuracy for this kind of problems that will allow us to analyze its behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data, creating descriptions, a dict containing a mapping from every picture name to its captioning.\n",
    "\n",
    "Most of these steps are taken from https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8\n",
    "\n",
    "Our goal is not to create the model but to visualize what is happening when used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len lines : 30000\n",
      "len lines : 35000\n",
      "len lines : 40000\n"
     ]
    }
   ],
   "source": [
    "with open(\"flickr_8k_train_dataset.txt\") as file:\n",
    "    lines = file.readlines()[1:]\n",
    "    \n",
    "print(f\"len lines : {len(lines)}\")\n",
    "\n",
    "with open(\"flickr_8k_test_dataset.txt\") as file:\n",
    "    lines += file.readlines()[1:]\n",
    "\n",
    "print(f\"len lines : {len(lines)}\")\n",
    "\n",
    "with open(\"flickr_8k_val_dataset.txt\") as file:\n",
    "    lines += file.readlines()[1:]\n",
    "    \n",
    "print(f\"len lines : {len(lines)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2513260012_03d33305cf.jpg\\t<start> A black dog is running after a white dog in the snow . <end>\\n'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = dict()\n",
    "mapping = dict()\n",
    "for line in lines:\n",
    "    # split line by white space\n",
    "    tokens = line.split()\n",
    "    \n",
    "    # take the first token as image id, the rest as description\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    \n",
    "    # extract filename from image id\n",
    "    image_id = image_id.split('.')[0]\n",
    "    \n",
    "    # convert description tokens back to string\n",
    "    image_desc = ' '.join(image_desc)\n",
    "    if image_id not in descriptions:\n",
    "        descriptions[image_id] = list()\n",
    "    descriptions[image_id].append(image_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare translation table for removing punctuation\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "for key, desc_list in descriptions.items():\n",
    "    for i in range(len(desc_list)):\n",
    "        desc = desc_list[i]\n",
    "        # tokenize\n",
    "        desc = desc.split()\n",
    "        # convert to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # remove punctuation from each token\n",
    "        desc = [w.translate(table) for w in desc]\n",
    "        # remove hanging 's' and 'a'\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        # remove tokens with numbers in them\n",
    "        desc = [word for word in desc if word.isalpha()]\n",
    "        # store as string\n",
    "        desc_list[i] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start black dog is running after white dog in the snow end',\n",
       " 'start black dog chasing brown dog through snow end',\n",
       " 'start two dogs chase each other across the snowy ground end',\n",
       " 'start two dogs play together in the snow end',\n",
       " 'start two dogs running through low lying body of water end']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions[[elem for elem in descriptions.keys()][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary Size: 8657\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for key in descriptions.keys():\n",
    "    [vocabulary.update(d.split()) for d in descriptions[key]]\n",
    "print('Original Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions, test_descriptions = train_test_split([elem for elem in descriptions], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_descriptions, val_descriptions = train_test_split(test_descriptions, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = dict( [ ( desc, descriptions[desc] ) for desc in train_descriptions] )\n",
    "test_descriptions = dict( [ ( desc, descriptions[desc] ) for desc in test_descriptions] )\n",
    "val_descriptions = dict( [ ( desc, descriptions[desc] ) for desc in val_descriptions] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_descriptions) + len(test_descriptions) + len(val_descriptions) == len(descriptions):\n",
    "    del(descriptions)\n",
    "else:\n",
    "    raise Error(\"Data is missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed words 1721 \n"
     ]
    }
   ],
   "source": [
    "# Create a list of all the training captions\n",
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)\n",
    "\n",
    "# Consider only words which occur at least 10 times in the corpus\n",
    "word_count_threshold = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold] + ['startseq' , 'endseq' , '0']\n",
    "\n",
    "print('preprocessed words %d ' % len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.pickle\" , \"wb\") as file:\n",
    "    pickle.Pickler(file).dump(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace start and end by 'startseq' and 'endseq' for each sentence that starts with 'start' or ends with 'end'\n",
    "def replace_starting_seq(s, seq, new_seq):\n",
    "    return s if not s.startswith(seq) else new_seq + s[len(seq):]\n",
    "def replace_ending_seq(s, seq, new_seq):\n",
    "    return s if not s.endswith(seq) else  s[:-len(seq)] + new_seq\n",
    "\n",
    "train_descriptions = dict( [ ( key , list( map( lambda x : replace_starting_seq( replace_ending_seq( x , 'end' , 'endseq' ) , 'start' , 'startseq' ) , descs ) ) ) for key , descs in train_descriptions.items() ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the InceptionV3 model trained on imagenet data\n",
    "model = InceptionV3(weights='imagenet')\n",
    "# Remove the last layer (output softmax layer) from the inception v3\n",
    "model_new = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2387197355_237f6f41ee.jpg'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_names = os.listdir(\"Flickr8k/Flicker8k_Dataset/\")\n",
    "images_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2387197355_237f6f41ee'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_names = [elem.split(\".\")[0] for elem in images_names]\n",
    "images_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = iter(train_descriptions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = [ ( tr_name , image.load_img(f\"Flickr8k/Flicker8k_Dataset/{tr_name}.jpg\", target_size=(299, 299)) ) for tr_name in train_descriptions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = [ ( te_name , image.load_img(f\"Flickr8k/Flicker8k_Dataset/{te_name}.jpg\", target_size=(299, 299)) ) for te_name in test_descriptions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs = [ ( val_name , image.load_img(f\"Flickr8k/Flicker8k_Dataset/{val_name}.jpg\", target_size=(299, 299)) ) for val_name in val_descriptions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_dico = dict( [ ( key , { 'descriptions' : train_descriptions[key] , 'features' : np.expand_dims( np.array( tr_d ) , axis=0 )} ) for  key , tr_d in train_imgs ] )\n",
    "\n",
    "test_imgs_dico = dict( [ ( key , { 'descriptions' : test_descriptions[key] , 'features' : np.expand_dims( np.array( te_d ) , axis=0 )} ) for  key , te_d in test_imgs ] )\n",
    "\n",
    "val_imgs_dico = dict( [ ( key , { 'descriptions' : val_descriptions[key] , 'features' : np.expand_dims( np.array( val_d ) , axis=0 )} ) for  key , val_d in val_imgs ] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for every image (key) the descriptions and the vector representation\n",
    "# Will be used during exploration of feature extractions from the original data\n",
    "\n",
    "with open('train_imgs.pickle' , 'wb') as file:\n",
    "    \n",
    "    pickler = pickle.Pickler( file )\n",
    "    pickler.dump( train_imgs_dico )\n",
    "    \n",
    "with open('test_imgs.pickle' , 'wb') as file:\n",
    "    \n",
    "    pickler = pickle.Pickler(file)\n",
    "    pickler.dump( test_imgs_dico )\n",
    "    \n",
    "with open('val_imgs.pickle' , 'wb') as file:\n",
    "    \n",
    "    pickler = pickle.Pickler(file)\n",
    "    pickler.dump( val_imgs_dico )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction using InceptionV3 ( transfer learning )\n",
    "\n",
    "This step is computationally heavy, can take several hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =  [  model_new.predict( np.expand_dims( image.img_to_array( tr_im ) , axis=0 ) ) for tr_im in train_imgs ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data =  [  model_new.predict( np.expand_dims( image.img_to_array( te_im ) , axis=0 ) ) for te_im in test_imgs ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data =  [  model_new.predict( np.expand_dims( image.img_to_array( val_im ) , axis=0 ) ) for val_im in val_imgs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dico = dict( [ ( key , { 'descriptions' : descs , 'features' : tr_d } ) for ( key , descs ) , tr_d in zip(train_descriptions.items() , train_data) ] )\n",
    "\n",
    "test_data_dico = dict( [ ( key , { 'descriptions' : descs , 'features' : te_d } ) for ( key , descs ) , te_d in zip(test_descriptions.items() , test_data) ] )\n",
    "\n",
    "val_data_dico = dict( [ ( key , { 'descriptions' : descs , 'features' : val_d } ) for ( key , descs ) , val_d in zip(val_descriptions.items() , val_data) ] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each image (key) with its descriptions (captions) and the encoded features vector\n",
    "\n",
    "with open('train_data_encoded.pickle' , 'wb') as file:\n",
    "    \n",
    "    pickler = pickle.Pickler( file )\n",
    "    pickler.dump( train_data_dico )\n",
    "    \n",
    "with open('test_data_encoded.pickle' , 'wb') as file:\n",
    "    \n",
    "    pickler = pickle.Pickler(file)\n",
    "    pickler.dump( test_data_dico )\n",
    "    \n",
    "with open('val_data_encoded.pickle' , 'wb') as file:\n",
    "    \n",
    "    pickler = pickle.Pickler(file)\n",
    "    pickler.dump( val_data_dico )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References: \n",
    "\n",
    "dataset : Flickr 8k\n",
    "\n",
    "code and code inspiration : https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
