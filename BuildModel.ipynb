{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalilcherif/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imageio import imread\n",
    "\n",
    "from keras.applications import InceptionV3\n",
    "from keras import Model\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding , Input , Dropout , Dense , LSTM , add\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem description\n",
    "\n",
    "Our goal is to show how neural networks behave during automated image captioning. We aim to understand what are the composants of the neural network that are activated and that participates in the recognition of an \"object\" in the image. \n",
    "\n",
    "In a Second step, we will try to see if we can reduce the size of the neural network using this information to produce a smaller neiral net with the same behaviour as the first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Our aim is not accuracy, we want a neural network with a decent accuracy for this kind of problems that will allow us to analyze its behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data, creating descriptions, a dict containing a mapping from every picture name to its captioning.\n",
    "\n",
    "Most of these steps are taken from https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8\n",
    "\n",
    "Our goal is not to create the model but to visualize what is happening when used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the InceptionV3 model trained on imagenet data\n",
    "model = InceptionV3(weights='imagenet')\n",
    "# Remove the last layer (output softmax layer) from the inception v3\n",
    "model_new = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2387197355_237f6f41ee.jpg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_names = os.listdir(\"Flickr8k/Flicker8k_Dataset/\")\n",
    "images_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2387197355_237f6f41ee'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_names = [elem.split(\".\")[0] for elem in images_names]\n",
    "images_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for elem in images_names:\n",
    "    if elem not in train_descriptions and elem not in test_descriptions and elem not in val_descriptions:        \n",
    "        i+=1\n",
    "        \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_data_encoded.pickle\" , \"rb\") as file:\n",
    "    unpickler = pickle.Unpickler(file)\n",
    "    train_data = unpickler.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = dict( [ ( key , val[\"descriptions\"] ) for key,val in train_data.items() ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.pickle\" , \"rb\") as file:\n",
    "    vocab = pickle.Unpickler(file).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoix[w] = ix\n",
    "    ixtoword[ix] = w\n",
    "    ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(descriptions):\n",
    "    return max( *[ max( *[ len( d.split() ) for d in desc ] ) for desc in descriptions.values() ] )\n",
    "\n",
    "def get_max_len2(descriptions):\n",
    "    return max( *[ len( d.split() ) for desc in descriptions.values() for d in desc ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_max_len2(train_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1721, 1721, 1721)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ixtoword) , len(wordtoix) , len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data ,wordtoix, max_length, num_photos_per_batch):\n",
    "    \n",
    "    descriptions =  dict( [ ( key , val['descriptions'] ) for key,val in data.items() ] )\n",
    "    photos = dict( [ ( key , ( val['features'] )) for key,val in data.items() ] )\n",
    "\n",
    "    # data generator, intended to be used in a call to model.fit_generator()\n",
    "    def __data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "        vocab_size = len( wordtoix )\n",
    "        X1, X2, y = list(), list(), list()\n",
    "        n=0\n",
    "        # loop for ever over images\n",
    "        while True:\n",
    "            for key, desc_list in descriptions.items():\n",
    "                n+=1\n",
    "                # retrieve the photo feature\n",
    "                photo = photos[key]\n",
    "                for desc in desc_list:\n",
    "                    # encode the sequence\n",
    "                    seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                    # split one sequence into multiple X, y pairs\n",
    "                    for i in range(1, len(seq)):\n",
    "                        # split into input and output pair\n",
    "                        in_seq, out_seq = seq[:i], seq[i]\n",
    "                        # pad input sequence\n",
    "                        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                        # encode output sequence\n",
    "                        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                        # store\n",
    "                        X1.append(photo)\n",
    "                        X2.append(in_seq)\n",
    "                        y.append(out_seq)\n",
    "                # yield the batch data\n",
    "                if n==num_photos_per_batch:\n",
    "                    yield [ [ np.array(X1), np.array(X2)], np.array(y) ]\n",
    "                    X1, X2, y = list(), list(), list()\n",
    "                    n=0\n",
    "                    \n",
    "    return __data_generator( descriptions , photos , wordtoix , max_length , num_photos_per_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n photos/batch : 5\n",
      "max sequence length : 34\n"
     ]
    }
   ],
   "source": [
    "num_photos_per_batch = 5\n",
    "max_length = get_max_len( train_descriptions )\n",
    "\n",
    "print(f'n photos/batch : {num_photos_per_batch}\\nmax sequence length : {max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = data_generator( train_data , wordtoix , max_length , num_photos_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../Downloads/glove.6B.200d.txt\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "glove_dict = dict( [ ( elem.split()[0] , np.asarray( elem.split()[1:] , dtype='float32' ) ) for elem in lines ] )\n",
    "del(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = dict( [ ( word , glove_dict.get( word , np.zeros( embedding_dim ) ) ) for word in wordtoix ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features input shape : (1, 2048)\n",
      "vocab size : 1721\n",
      "embedding dim : 200\n"
     ]
    }
   ],
   "source": [
    "features_input_shape = train_data[ next( iter( train_data.keys() ) ) ]['features'].shape\n",
    "vocab_size = len( wordtoix )\n",
    "\n",
    "print(f'features input shape : {features_input_shape}\\nvocab size : {vocab_size}\\nembedding dim : {embedding_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image feature extractor model\n",
    "inputs1 = Input( shape = features_input_shape )\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# partial caption sequence model\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder (feed forward) model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "# merge the two input models\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 2048)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 34, 200)      344200      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1, 2048)      0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 34, 200)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1, 256)       524544      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 256)          467968      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1, 256)       0           dense_4[0][0]                    \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1, 256)       65792       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1, 1721)      442297      dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,844,801\n",
      "Trainable params: 1,844,801\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = dict( [ ( wordtoix[key] , val) for key,val in embedding_matrix.items() ] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights( [ np.array([ embedding_matrix[i] for i in sorted( embedding_matrix.keys() ) ] ) ] )\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n photos/batch : 30\n",
      "steps : 213\n"
     ]
    }
   ],
   "source": [
    "num_photos_per_batch = 30\n",
    "steps = len(train_descriptions)//num_photos_per_batch\n",
    "\n",
    "print(f'n photos/batch : {num_photos_per_batch}\\nsteps : {steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = data_generator( train_data , wordtoix , max_length , num_photos_per_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length val data : 800\n"
     ]
    }
   ],
   "source": [
    "with open('val_data_encoded.pickle' , 'rb') as file:\n",
    "    unpickler = pickle.Unpickler(file)\n",
    "    val_data = unpickler.load()\n",
    "\n",
    "print(f'length val data : {len(val_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = next( data_generator( val_data , wordtoix , max_length , num_photos_per_batch = len( val_data ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model, takes several hours\n",
    "\n",
    "Better load the model and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    num_photos_per_batch = (2**epochs) * 5 / (2**epoch)\n",
    "    steps = len(train_descriptions)//num_photos_per_batch\n",
    "    model.optimizer.lr = 0.001 / (2**epoch)\n",
    "    model.fit_generator(train_data_gen , epochs=1, steps_per_epoch=steps, verbose=1 )\n",
    "    \n",
    "model.save(f'model__lr_{model.optimizer.lr}.h5')\n",
    "model.save_weights('my_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References: \n",
    "\n",
    "dataset : Flickr 8k Images\n",
    "\n",
    "code inspiration and code : https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
